{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Editing Benchmark Evaluation\n",
        "\n",
        "This notebook performs benchmark evaluation for knowledge editing tasks using CounterFact dataset.\n",
        "\n",
        "## Algorithms Supported\n",
        "- ICL (In-Context Learning)\n",
        "- ROME (WIP)\n",
        "- MEMIT (WIP)\n",
        "- Fine-tuning (WIP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bumjin/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Add frontend directory to path to import evaluation modules\n",
        "frontend_dir = os.path.join(os.path.dirname(os.getcwd()), 'frontend')\n",
        "if os.path.exists(frontend_dir):\n",
        "    sys.path.insert(0, frontend_dir)\n",
        "else:\n",
        "    # If running from root directory\n",
        "    sys.path.insert(0, 'frontend')\n",
        "\n",
        "from eval import CounterfactEvaluator, generate_text\n",
        "from algorithms.icl import ICLAlgorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "MODEL_NAME = \"google/gemma-2-2b\"  # Change to your desired model\n",
        "CUDA_VISIBLE_DEVICES = \"0\"  # GPU device(s) to use\n",
        "\n",
        "# Generation Parameters\n",
        "MAX_NEW_TOKENS = 15\n",
        "TEMPERATURE = 0.7\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Dataset Configuration\n",
        "DATASET_NAME = \"counterfact\"  # Options: counterfact, mquake-cf, wikiupdate\n",
        "USE_SUBSAMPLING = True  # Use subset of data for faster evaluation\n",
        "SUBSAMPLING_SIZE = 10  # Number of samples to evaluate\n",
        "SUBSAMPLING_SEED = 42  # Random seed for subsampling\n",
        "\n",
        "# Algorithm\n",
        "ALGORITHM = \"icl\"  # Options: icl, rome, memit, ft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: google/gemma-2-2b\n",
            "CUDA_VISIBLE_DEVICES: 0\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/home/bumjin/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/bumjin/anaconda3/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "2025-12-13 10:06:36.506266: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-13 10:06:36.520943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-12-13 10:06:36.539233: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-12-13 10:06:36.544690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-12-13 10:06:36.556151: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-12-13 10:06:37.250888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18c29cce3e1845409f393ebd7f99f901",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully on device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set CUDA devices\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"Model loaded successfully on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Evaluator and Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total cases in dataset: 19728\n",
            "Subsampling: Selected 10 cases from 19728 total cases (seed=42)\n",
            "Initialized ICL algorithm\n"
          ]
        }
      ],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = CounterfactEvaluator()\n",
        "\n",
        "# Get dataset\n",
        "if hasattr(evaluator.data, '__iter__') and not isinstance(evaluator.data, dict):\n",
        "    data_list = list(evaluator.data)\n",
        "else:\n",
        "    data_list = evaluator.data if isinstance(evaluator.data, list) else list(evaluator.data.values())\n",
        "\n",
        "print(f\"Total cases in dataset: {len(data_list)}\")\n",
        "\n",
        "# Apply subsampling if requested\n",
        "if USE_SUBSAMPLING and SUBSAMPLING_SIZE < len(data_list):\n",
        "    random.seed(SUBSAMPLING_SEED)\n",
        "    data_list = random.sample(data_list, SUBSAMPLING_SIZE)\n",
        "    print(f\"Subsampling: Selected {len(data_list)} cases from {len(evaluator.data)} total cases (seed={SUBSAMPLING_SEED})\")\n",
        "\n",
        "# Temporarily replace evaluator data with subsampled data\n",
        "original_data = evaluator.data\n",
        "evaluator.data = data_list\n",
        "\n",
        "# Initialize algorithm\n",
        "if ALGORITHM == \"icl\":\n",
        "    algorithm = ICLAlgorithm(model=model, tokenizer=tokenizer)\n",
        "    print(f\"Initialized {ALGORITHM.upper()} algorithm\")\n",
        "else:\n",
        "    raise ValueError(f\"Algorithm {ALGORITHM} not yet implemented. Use 'icl' for now.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation with 10 cases...\n",
            "Parameters: max_new_tokens=15, temperature=0.7, batch_size=16\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Progress tracking callback\n",
        "def progress_callback(case_idx, case_result, total):\n",
        "    \"\"\"Print progress during evaluation\"\"\"\n",
        "    if (case_idx + 1) % 10 == 0 or (case_idx + 1) == total:\n",
        "        progress = ((case_idx + 1) / total * 100)\n",
        "        subject = case_result.get('subject', 'N/A')\n",
        "        print(f\"Progress: {case_idx + 1}/{total} ({progress:.1f}%) - Current: {subject}\")\n",
        "\n",
        "print(f\"Starting evaluation with {len(data_list)} cases...\")\n",
        "print(f\"Parameters: max_new_tokens={MAX_NEW_TOKENS}, temperature={TEMPERATURE}, batch_size={BATCH_SIZE}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluator.evaluate(\n",
        "    algorithm=algorithm,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    temperature=TEMPERATURE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    progress_callback=progress_callback\n",
        ")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"Evaluation completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
        "print(f\"Total cases evaluated: {len(results)}\")\n",
        "\n",
        "# Restore original data\n",
        "evaluator.data = original_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Calculate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(results):\n",
        "    \"\"\"Calculate aggregate metrics from evaluation results\"\"\"\n",
        "    all_reliability_scores = []\n",
        "    all_portability_scores = []\n",
        "    all_locality_scores = []\n",
        "    \n",
        "    for case_id, case_result in results.items():\n",
        "        eval_data = case_result.get('eval', {})\n",
        "        \n",
        "        # Reliability: paraphrase_prompts scores\n",
        "        paraphrase_scores = eval_data.get('icl_scores_paraphrase_prompts', [])\n",
        "        all_reliability_scores.extend(paraphrase_scores)\n",
        "        \n",
        "        # Portability: generation_prompts scores\n",
        "        generation_scores = eval_data.get('icl_scores_generation_prompts', [])\n",
        "        all_portability_scores.extend(generation_scores)\n",
        "        \n",
        "        # Locality: neighborhood_prompts + attribute_prompts scores\n",
        "        neighborhood_scores = eval_data.get('icl_scores_neighborhood_prompts', [])\n",
        "        attribute_scores = eval_data.get('icl_scores_attribute_prompts', [])\n",
        "        all_locality_scores.extend(neighborhood_scores)\n",
        "        all_locality_scores.extend(attribute_scores)\n",
        "    \n",
        "    metrics = {\n",
        "        'reliability': sum(all_reliability_scores) / len(all_reliability_scores) if all_reliability_scores else 0.0,\n",
        "        'portability': sum(all_portability_scores) / len(all_portability_scores) if all_portability_scores else 0.0,\n",
        "        'locality': sum(all_locality_scores) / len(all_locality_scores) if all_locality_scores else 0.0,\n",
        "        'total_cases': len(results)\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "metrics = calculate_metrics(results)\n",
        "\n",
        "print(\"Overall Metrics:\")\n",
        "print(f\"  Reliability: {metrics['reliability']*100:.2f}%\")\n",
        "print(f\"  Portability: {metrics['portability']*100:.2f}%\")\n",
        "print(f\"  Locality: {metrics['locality']*100:.2f}%\")\n",
        "print(f\"  Total Cases: {metrics['total_cases']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Detailed Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Convert results to DataFrame for easier analysis\n",
        "case_results = []\n",
        "for case_id, case_result in results.items():\n",
        "    eval_data = case_result.get('eval', {})\n",
        "    \n",
        "    # Calculate average scores for each metric\n",
        "    pr_scores = eval_data.get('icl_scores_paraphrase_prompts', [])\n",
        "    gr_scores = eval_data.get('icl_scores_generation_prompts', [])\n",
        "    nr_scores = eval_data.get('icl_scores_neighborhood_prompts', [])\n",
        "    at_scores = eval_data.get('icl_scores_attribute_prompts', [])\n",
        "    \n",
        "    case_results.append({\n",
        "        'Case ID': case_id,\n",
        "        'Subject': case_result.get('subject', ''),\n",
        "        'Target New': case_result.get('target_new', ''),\n",
        "        'Target Old': case_result.get('target_old', ''),\n",
        "        'PR (Paraphrase Reliability)': np.mean(pr_scores) if pr_scores else 0.0,\n",
        "        'GR (Generation Portability)': np.mean(gr_scores) if gr_scores else 0.0,\n",
        "        'NR (Neighborhood Locality)': np.mean(nr_scores) if nr_scores else 0.0,\n",
        "        'AT (Attribute Locality)': np.mean(at_scores) if at_scores else 0.0,\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(case_results)\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"Score Statistics:\")\n",
        "print(df[['PR (Paraphrase Reliability)', 'GR (Generation Portability)', \n",
        "          'NR (Neighborhood Locality)', 'AT (Attribute Locality)']].describe())\n",
        "\n",
        "# Display first few results\n",
        "print(\"\\nFirst 10 Cases:\")\n",
        "display(df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot score distributions\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle('Score Distributions', fontsize=16)\n",
        "\n",
        "metrics_list = ['PR (Paraphrase Reliability)', 'GR (Generation Portability)', \n",
        "                'NR (Neighborhood Locality)', 'AT (Attribute Locality)']\n",
        "\n",
        "for idx, metric in enumerate(metrics_list):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    ax.hist(df[metric], bins=20, alpha=0.7, edgecolor='black')\n",
        "    ax.set_title(metric)\n",
        "    ax.set_xlabel('Score')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.axvline(df[metric].mean(), color='red', linestyle='--', label=f'Mean: {df[metric].mean():.3f}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot overall metrics\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "metric_names = ['Reliability', 'Portability', 'Locality']\n",
        "metric_values = [metrics['reliability'], metrics['portability'], metrics['locality']]\n",
        "bars = ax.bar(metric_names, [v*100 for v in metric_values], color=['#4CAF50', '#2196F3', '#FF9800'])\n",
        "ax.set_ylabel('Score (%)')\n",
        "ax.set_title('Overall Metrics')\n",
        "ax.set_ylim(0, 100)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, metric_values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{value*100:.2f}%',\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON\n",
        "from datetime import datetime\n",
        "\n",
        "output_data = {\n",
        "    'config': {\n",
        "        'model_name': MODEL_NAME,\n",
        "        'algorithm': ALGORITHM,\n",
        "        'max_new_tokens': MAX_NEW_TOKENS,\n",
        "        'temperature': TEMPERATURE,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'dataset': DATASET_NAME,\n",
        "        'use_subsampling': USE_SUBSAMPLING,\n",
        "        'subsampling_size': SUBSAMPLING_SIZE if USE_SUBSAMPLING else None,\n",
        "        'subsampling_seed': SUBSAMPLING_SEED if USE_SUBSAMPLING else None,\n",
        "    },\n",
        "    'metrics': metrics,\n",
        "    'results': results,\n",
        "    'elapsed_time': elapsed_time\n",
        "}\n",
        "\n",
        "# Create output filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_filename = f\"benchmark_results_{DATASET_NAME}_{ALGORITHM}_{timestamp}.json\"\n",
        "\n",
        "with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Results saved to: {output_filename}\")\n",
        "\n",
        "# Also save DataFrame as CSV\n",
        "csv_filename = f\"benchmark_results_{DATASET_NAME}_{ALGORITHM}_{timestamp}.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV saved to: {csv_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Examine Individual Cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View detailed results for a specific case\n",
        "case_id_to_examine = list(results.keys())[0]  # Change to examine different case\n",
        "case_result = results[case_id_to_examine]\n",
        "\n",
        "print(f\"Case ID: {case_id_to_examine}\")\n",
        "print(f\"Subject: {case_result.get('subject', 'N/A')}\")\n",
        "print(f\"Target Old: {case_result.get('target_old', 'N/A')}\")\n",
        "print(f\"Target New: {case_result.get('target_new', 'N/A')}\")\n",
        "print(f\"Prompt: {case_result.get('prompt', 'N/A')}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Evaluation Results:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "eval_data = case_result.get('eval', {})\n",
        "\n",
        "# Display generated texts and scores\n",
        "for test_type in ['paraphrase_prompts', 'generation_prompts', 'neighborhood_prompts', 'attribute_prompts']:\n",
        "    generated_texts = eval_data.get(f'icl_{test_type}', [])\n",
        "    scores = eval_data.get(f'icl_scores_{test_type}', [])\n",
        "    \n",
        "    if generated_texts:\n",
        "        print(f\"\\n{test_type.replace('_', ' ').title()}:\")\n",
        "        for i, (text, score) in enumerate(zip(generated_texts, scores)):\n",
        "            print(f\"  [{i+1}] Score: {score}, Generated: {text[:100]}...\")\n",
        "\n",
        "# Pretty print full JSON for this case\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Full JSON:\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(case_result, indent=2, ensure_ascii=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
